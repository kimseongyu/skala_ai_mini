{
  "timestamp": "20250520_012125",
  "data": {
    "AI": [
      {
        "title": "AI Thinking: A framework for rethinking artificial intelligence in practice",
        "summary": "Artificial intelligence is transforming the way we work with information\nacross disciplines and practical contexts. A growing range of disciplines are\nnow involved in studying, developing, and assessing the use of AI in practice,\nbut these disciplines often employ conflicting understandings of what AI is and\nwhat is involved in its use. New, interdisciplinary approaches are needed to\nbridge competing conceptualisations of AI in practice and help shape the future\nof AI use. I propose a novel conceptual framework called AI Thinking, which\nmodels key decisions and considerations involved in AI use across disciplinary\nperspectives. The AI Thinking model addresses five practice-based competencies\ninvolved in applying AI in context: motivating AI use in information processes,\nformulating AI methods, assessing available tools and technologies, selecting\nappropriate data, and situating AI in the sociotechnical contexts it is used\nin. A hypothetical case study is provided to illustrate the application of AI\nThinking in practice. This article situates AI Thinking in broader\ncross-disciplinary discourses of AI, including its connections to ongoing\ndiscussions around AI literacy and AI-driven innovation. AI Thinking can help\nto bridge divides between academic disciplines and diverse contexts of AI use,\nand to reshape the future of AI in practice."
      },
      {
        "title": "Intersymbolic AI: Interlinking Symbolic AI and Subsymbolic AI",
        "summary": "This perspective piece calls for the study of the new field of Intersymbolic\nAI, by which we mean the combination of symbolic AI, whose building blocks have\ninherent significance/meaning, with subsymbolic AI, whose entirety creates\nsignificance/effect despite the fact that individual building blocks escape\nmeaning. Canonical kinds of symbolic AI are logic, games and planning.\nCanonical kinds of subsymbolic AI are (un)supervised machine and reinforcement\nlearning. Intersymbolic AI interlinks the worlds of symbolic AI with its\ncompositional symbolic significance and meaning and of subsymbolic AI with its\nsummative significance or effect to enable culminations of insights from both\nworlds by going between and across symbolic AI insights with subsymbolic AI\ntechniques that are being helped by symbolic AI principles. For example,\nIntersymbolic AI may start with symbolic AI to understand a dynamic system,\ncontinue with subsymbolic AI to learn its control, and end with symbolic AI to\nsafely use the outcome of the learned subsymbolic AI controller in the dynamic\nsystem. The way Intersymbolic AI combines both symbolic and subsymbolic AI to\nincrease the effectiveness of AI compared to either kind of AI alone is likened\nto the way that the combination of both conscious and subconscious thought\nincreases the effectiveness of human thought compared to either kind of thought\nalone. Some successful contributions to the Intersymbolic AI paradigm are\nsurveyed here but many more are considered possible by advancing Intersymbolic\nAI."
      },
      {
        "title": "Overconfident and Unconfident AI Hinder Human-AI Collaboration",
        "summary": "AI transparency is a central pillar of responsible AI deployment and\neffective human-AI collaboration. A critical approach is communicating\nuncertainty, such as displaying AI's confidence level, or its correctness\nlikelihood (CL), to users. However, these confidence levels are often\nuncalibrated, either overestimating or underestimating actual CL, posing risks\nand harms to human-AI collaboration. This study examines the effects of\nuncalibrated AI confidence on users' trust in AI, AI advice adoption, and\ncollaboration outcomes. We further examined the impact of increased\ntransparency, achieved through trust calibration support, on these outcomes.\nOur results reveal that uncalibrated AI confidence leads to both the misuse of\noverconfident AI and disuse of unconfident AI, thereby hindering outcomes of\nhuman-AI collaboration. Deficiency of trust calibration support exacerbates\nthis issue by making it harder to detect uncalibrated confidence, promoting\nmisuse and disuse of AI. Conversely, trust calibration support aids in\nrecognizing uncalibration and reducing misuse, but it also fosters distrust and\ncauses disuse of AI. Our findings highlight the importance of AI confidence\ncalibration for enhancing human-AI collaboration and suggest directions for AI\ndesign and regulation."
      },
      {
        "title": "Supporting AI/ML Security Workers through an Adversarial Techniques, Tools, and Common Knowledge (AI/ML ATT&CK) Framework",
        "summary": "This paper focuses on supporting AI/ML Security Workers -- professionals\ninvolved in the development and deployment of secure AI-enabled software\nsystems. It presents AI/ML Adversarial Techniques, Tools, and Common Knowledge\n(AI/ML ATT&CK) framework to enable AI/ML Security Workers intuitively to\nexplore offensive and defensive tactics."
      },
      {
        "title": "Navigating Fairness: Practitioners' Understanding, Challenges, and Strategies in AI/ML Development",
        "summary": "The rise in the use of AI/ML applications across industries has sparked more\ndiscussions about the fairness of AI/ML in recent times. While prior research\non the fairness of AI/ML exists, there is a lack of empirical studies focused\non understanding the perspectives and experiences of AI practitioners in\ndeveloping a fair AI/ML system. Understanding AI practitioners' perspectives\nand experiences on the fairness of AI/ML systems are important because they are\ndirectly involved in its development and deployment and their insights can\noffer valuable real-world perspectives on the challenges associated with\nensuring fairness in AI/ML systems. We conducted semi-structured interviews\nwith 22 AI practitioners to investigate their understanding of what a 'fair\nAI/ML' is, the challenges they face in developing a fair AI/ML system, the\nconsequences of developing an unfair AI/ML system, and the strategies they\nemploy to ensure AI/ML system fairness. We developed a framework showcasing the\nrelationship between AI practitioners' understanding of 'fair AI/ML' system and\n(i) their challenges in its development, (ii) the consequences of developing an\nunfair AI/ML system, and (iii) strategies used to ensure AI/ML system fairness.\nBy exploring AI practitioners' perspectives and experiences, this study\nprovides actionable insights to enhance AI/ML fairness, which may promote\nfairer systems, reduce bias, and foster public trust in AI technologies.\nAdditionally, we also identify areas for further investigation and offer\nrecommendations to aid AI practitioners and AI companies in navigating\nfairness."
      },
      {
        "title": "A Bibliometric View of AI Ethics Development",
        "summary": "Artificial Intelligence (AI) Ethics is a nascent yet critical research field.\nRecent developments in generative AI and foundational models necessitate a\nrenewed look at the problem of AI Ethics. In this study, we perform a\nbibliometric analysis of AI Ethics literature for the last 20 years based on\nkeyword search. Our study reveals a three-phase development in AI Ethics,\nnamely an incubation phase, making AI human-like machines phase, and making AI\nhuman-centric machines phase. We conjecture that the next phase of AI ethics is\nlikely to focus on making AI more machine-like as AI matches or surpasses\nhumans intellectually, a term we coin as \"machine-like human\"."
      },
      {
        "title": "Threefold model for AI Readiness: A Case Study with Finnish Healthcare SMEs",
        "summary": "This study examines AI adoption among Finnish healthcare SMEs through\nsemi-structured interviews with six health-tech companies. We identify three AI\nengagement categories: AI-curious (exploring AI), AI-embracing (integrating\nAI), and AI-catering (providing AI solutions). Our proposed threefold model\nhighlights key adoption barriers, including regulatory complexities, technical\nexpertise gaps, and financial constraints. While SMEs recognize AI's potential,\nmost remain in early adoption stages. We provide actionable recommendations to\naccelerate AI integration, focusing on regulatory reforms, talent development,\nand inter-company collaboration, offering valuable insights for healthcare\norganizations, policymakers, and researchers."
      },
      {
        "title": "Towards a Capability Assessment Model for the Comprehension and Adoption of AI in Organisations",
        "summary": "The comprehension and adoption of Artificial Intelligence (AI) are beset with\npractical and ethical problems. This article presents a 5-level AI Capability\nAssessment Model (AI-CAM) and a related AI Capabilities Matrix (AI-CM) to\nassist practitioners in AI comprehension and adoption. These practical tools\nwere developed with business executives, technologists, and other\norganisational stakeholders in mind. They are founded on a comprehensive\nconception of AI compared to those in other AI adoption models and are also\nopen-source artefacts. Thus, the AI-CAM and AI-CM present an accessible\nresource to help inform organisational decision-makers on the capability\nrequirements for (1) AI-based data analytics use cases based on machine\nlearning technologies; (2) Knowledge representation to engineer and represent\ndata, information and knowledge using semantic technologies; and (3) AI-based\nsolutions that seek to emulate human reasoning and decision-making. The AI-CAM\ncovers the core capability dimensions (business, data, technology,\norganisation, AI skills, risks, and ethical considerations) required at the\nfive capability maturity levels to achieve optimal use of AI in organisations."
      },
      {
        "title": "Improving Health Professionals' Onboarding with AI and XAI for Trustworthy Human-AI Collaborative Decision Making",
        "summary": "With advanced AI/ML, there has been growing research on explainable AI (XAI)\nand studies on how humans interact with AI and XAI for effective human-AI\ncollaborative decision-making. However, we still have a lack of understanding\nof how AI systems and XAI should be first presented to users without technical\nbackgrounds. In this paper, we present the findings of semi-structured\ninterviews with health professionals (n=12) and students (n=4) majoring in\nmedicine and health to study how to improve onboarding with AI and XAI. For the\ninterviews, we built upon human-AI interaction guidelines to create onboarding\nmaterials of an AI system for stroke rehabilitation assessment and AI\nexplanations and introduce them to the participants. Our findings reveal that\nbeyond presenting traditional performance metrics on AI, participants desired\nbenchmark information, the practical benefits of AI, and interaction trials to\nbetter contextualize AI performance, and refine the objectives and performance\nof AI. Based on these findings, we highlight directions for improving\nonboarding with AI and XAI and human-AI collaborative decision-making."
      }
    ],
    "LLM": [
      {
        "title": "Trustworthy and Efficient LLMs Meet Databases",
        "summary": "In the rapidly evolving AI era with large language models (LLMs) at the core,\nmaking LLMs more trustworthy and efficient, especially in output generation\n(inference), has gained significant attention. This is to reduce plausible but\nfaulty LLM outputs (a.k.a hallucinations) and meet the highly increased\ninference demands. This tutorial explores such efforts and makes them\ntransparent to the database community. Understanding these efforts is essential\nin harnessing LLMs in database tasks and adapting database techniques to LLMs.\nFurthermore, we delve into the synergy between LLMs and databases, highlighting\nnew opportunities and challenges in their intersection. This tutorial aims to\nshare with database researchers and practitioners essential concepts and\nstrategies around LLMs, reduce the unfamiliarity of LLMs, and inspire joining\nin the intersection between LLMs and databases."
      },
      {
        "title": "Large Language Models as Software Components: A Taxonomy for LLM-Integrated Applications",
        "summary": "Large Language Models (LLMs) have become widely adopted recently. Research\nexplores their use both as autonomous agents and as tools for software\nengineering. LLM-integrated applications, on the other hand, are software\nsystems that leverage an LLM to perform tasks that would otherwise be\nimpossible or require significant coding effort. While LLM-integrated\napplication engineering is emerging as new discipline, its terminology,\nconcepts and methods need to be established. This study provides a taxonomy for\nLLM-integrated applications, offering a framework for analyzing and describing\nthese systems. It also demonstrates various ways to utilize LLMs in\napplications, as well as options for implementing such integrations.\n  Following established methods, we analyze a sample of recent LLM-integrated\napplications to identify relevant dimensions. We evaluate the taxonomy by\napplying it to additional cases. This review shows that applications integrate\nLLMs in numerous ways for various purposes. Frequently, they comprise multiple\nLLM integrations, which we term ``LLM components''. To gain a clear\nunderstanding of an application's architecture, we examine each LLM component\nseparately. We identify thirteen dimensions along which to characterize an LLM\ncomponent, including the LLM skills leveraged, the format of the output, and\nmore. LLM-integrated applications are described as combinations of their LLM\ncomponents. We suggest a concise representation using feature vectors for\nvisualization.\n  The taxonomy is effective for describing LLM-integrated applications. It can\ncontribute to theory building in the nascent field of LLM-integrated\napplication engineering and aid in developing such systems. Researchers and\npractitioners explore numerous creative ways to leverage LLMs in applications.\nThough challenges persist, integrating LLMs may revolutionize the way software\nsystems are built."
      },
      {
        "title": "Parrot: Efficient Serving of LLM-based Applications with Semantic Variable",
        "summary": "The rise of large language models (LLMs) has enabled LLM-based applications\n(a.k.a. AI agents or co-pilots), a new software paradigm that combines the\nstrength of LLM and conventional software. Diverse LLM applications from\ndifferent tenants could design complex workflows using multiple LLM requests to\naccomplish one task. However, they have to use the over-simplified\nrequest-level API provided by today's public LLM services, losing essential\napplication-level information. Public LLM services have to blindly optimize\nindividual LLM requests, leading to sub-optimal end-to-end performance of LLM\napplications.\n  This paper introduces Parrot, an LLM service system that focuses on the\nend-to-end experience of LLM-based applications. Parrot proposes Semantic\nVariable, a unified abstraction to expose application-level knowledge to public\nLLM services. A Semantic Variable annotates an input/output variable in the\nprompt of a request, and creates the data pipeline when connecting multiple LLM\nrequests, providing a natural way to program LLM applications. Exposing\nSemantic Variables to the public LLM service allows it to perform conventional\ndata flow analysis to uncover the correlation across multiple LLM requests.\nThis correlation opens a brand-new optimization space for the end-to-end\nperformance of LLM-based applications. Extensive evaluations demonstrate that\nParrot can achieve up to an order-of-magnitude improvement for popular and\npractical use cases of LLM applications."
      },
      {
        "title": "A Survey of Large Language Models for Code: Evolution, Benchmarking, and Future Trends",
        "summary": "General large language models (LLMs), represented by ChatGPT, have\ndemonstrated significant potential in tasks such as code generation in software\nengineering. This has led to the development of specialized LLMs for software\nengineering, known as Code LLMs. A considerable portion of Code LLMs is derived\nfrom general LLMs through model fine-tuning. As a result, Code LLMs are often\nupdated frequently and their performance can be influenced by the base LLMs.\nHowever, there is currently a lack of systematic investigation into Code LLMs\nand their performance. In this study, we conduct a comprehensive survey and\nanalysis of the types of Code LLMs and their differences in performance\ncompared to general LLMs. We aim to address three questions: (1) What LLMs are\nspecifically designed for software engineering tasks, and what is the\nrelationship between these Code LLMs? (2) Do Code LLMs really outperform\ngeneral LLMs in software engineering tasks? (3) Which LLMs are more proficient\nin different software engineering tasks? To answer these questions, we first\ncollect relevant literature and work from five major databases and open-source\ncommunities, resulting in 134 works for analysis. Next, we categorize the Code\nLLMs based on their publishers and examine their relationships with general\nLLMs and among themselves. Furthermore, we investigate the performance\ndifferences between general LLMs and Code LLMs in various software engineering\ntasks to demonstrate the impact of base models and Code LLMs. Finally, we\ncomprehensively maintained the performance of LLMs across multiple mainstream\nbenchmarks to identify the best-performing LLMs for each software engineering\ntask. Our research not only assists developers of Code LLMs in choosing base\nmodels for the development of more advanced LLMs but also provides insights for\npractitioners to better understand key improvement directions for Code LLMs."
      },
      {
        "title": "LLM Online Spatial-temporal Signal Reconstruction Under Noise",
        "summary": "This work introduces the LLM Online Spatial-temporal Reconstruction (LLM-OSR)\nframework, which integrates Graph Signal Processing (GSP) and Large Language\nModels (LLMs) for online spatial-temporal signal reconstruction. The LLM-OSR\nutilizes a GSP-based spatial-temporal signal handler to enhance graph signals\nand employs LLMs to predict missing values based on spatiotemporal patterns.\nThe performance of LLM-OSR is evaluated on traffic and meteorological datasets\nunder varying Gaussian noise levels. Experimental results demonstrate that\nutilizing GPT-4-o mini within the LLM-OSR is accurate and robust under Gaussian\nnoise conditions. The limitations are discussed along with future research\ninsights, emphasizing the potential of combining GSP techniques with LLMs for\nsolving spatiotemporal prediction tasks."
      },
      {
        "title": "What Limits LLM-based Human Simulation: LLMs or Our Design?",
        "summary": "We argue that advancing LLM-based human simulation requires addressing both\nLLM's inherent limitations and simulation framework design challenges. Recent\nstudies have revealed significant gaps between LLM-based human simulations and\nreal-world observations, highlighting these dual challenges. To address these\ngaps, we present a comprehensive analysis of LLM limitations and our design\nissues, proposing targeted solutions for both aspects. Furthermore, we explore\nfuture directions that address both challenges simultaneously, particularly in\ndata collection, LLM generation, and evaluation. To support further research in\nthis field, we provide a curated collection of LLM-based human simulation\nresources.\\footnote{https://github.com/Persdre/llm-human-simulation}"
      },
      {
        "title": "Asynchronous LLM Function Calling",
        "summary": "Large language models (LLMs) use function calls to interface with external\ntools and data source. However, the current approach to LLM function calling is\ninherently synchronous, where each call blocks LLM inference, limiting LLM\noperation and concurrent function execution. In this work, we propose AsyncLM,\na system for asynchronous LLM function calling. AsyncLM improves LLM's\noperational efficiency by enabling LLMs to generate and execute function calls\nconcurrently. Instead of waiting for each call's completion, AsyncLM introduces\nan interrupt mechanism to asynchronously notify the LLM in-flight when function\ncalls return. We design an in-context protocol for function calls and\ninterrupts, provide fine-tuning strategy to adapt LLMs to the interrupt\nsemantics, and implement these mechanisms efficiently on LLM inference process.\nWe demonstrate that AsyncLM can reduce end-to-end task completion latency from\n1.6x-5.4x compared to synchronous function calling on a set of benchmark tasks\nin the Berkeley function calling leaderboard (BFCL). Furthermore, we discuss\nhow interrupt mechanisms can be extended to enable novel human-LLM or LLM-LLM\ninteractions."
      },
      {
        "title": "Multi-LLM Text Summarization",
        "summary": "In this work, we propose a Multi-LLM summarization framework, and investigate\ntwo different multi-LLM strategies including centralized and decentralized. Our\nmulti-LLM summarization framework has two fundamentally important steps at each\nround of conversation: generation and evaluation. These steps are different\ndepending on whether our multi-LLM decentralized summarization is used or\ncentralized. In both our multi-LLM decentralized and centralized strategies, we\nhave k different LLMs that generate diverse summaries of the text. However,\nduring evaluation, our multi-LLM centralized summarization approach leverages a\nsingle LLM to evaluate the summaries and select the best one whereas k LLMs are\nused for decentralized multi-LLM summarization. Overall, we find that our\nmulti-LLM summarization approaches significantly outperform the baselines that\nleverage only a single LLM by up to 3x. These results indicate the\neffectiveness of multi-LLM approaches for summarization."
      },
      {
        "title": "A Survey of Large Language Models on Generative Graph Analytics: Query, Learning, and Applications",
        "summary": "A graph is a fundamental data model to represent various entities and their\ncomplex relationships in society and nature, such as social networks,\ntransportation networks, financial networks, and biomedical systems. Recently,\nlarge language models (LLMs) have showcased a strong generalization ability to\nhandle various NLP and multi-mode tasks to answer users' arbitrary questions\nand specific-domain content generation. Compared with graph learning models,\nLLMs enjoy superior advantages in addressing the challenges of generalizing\ngraph tasks by eliminating the need for training graph learning models and\nreducing the cost of manual annotation. In this survey, we conduct a\ncomprehensive investigation of existing LLM studies on graph data, which\nsummarizes the relevant graph analytics tasks solved by advanced LLM models and\npoints out the existing remaining challenges and future directions.\nSpecifically, we study the key problems of LLM-based generative graph analytics\n(LLM-GGA) with three categories: LLM-based graph query processing (LLM-GQP),\nLLM-based graph inference and learning (LLM-GIL), and graph-LLM-based\napplications. LLM-GQP focuses on an integration of graph analytics techniques\nand LLM prompts, including graph understanding and knowledge graph (KG) based\naugmented retrieval, while LLM-GIL focuses on learning and reasoning over\ngraphs, including graph learning, graph-formed reasoning and graph\nrepresentation. We summarize the useful prompts incorporated into LLM to handle\ndifferent graph downstream tasks. Moreover, we give a summary of LLM model\nevaluation, benchmark datasets/tasks, and a deep pro and cons analysis of LLM\nmodels. We also explore open problems and future directions in this exciting\ninterdisciplinary research area of LLMs and graph analytics."
      },
      {
        "title": "Systematic Evaluation of LLM-as-a-Judge in LLM Alignment Tasks: Explainable Metrics and Diverse Prompt Templates",
        "summary": "LLM-as-a-Judge has been widely applied to evaluate and compare different LLM\nalignmnet approaches (e.g., RLHF and DPO). However, concerns regarding its\nreliability have emerged, due to LLM judges' biases and inconsistent\ndecision-making. Previous research has developed evaluation frameworks to\nassess reliability of LLM judges and their alignment with human preferences.\nHowever, the employed evaluation metrics often lack adequate explainability and\nfail to address LLM internal inconsistency. Additionally, existing studies\ninadequately explore the impact of various prompt templates when applying\nLLM-as-a-Judge methods, leading to potentially inconsistent comparisons between\ndifferent alignment algorithms. In this work, we systematically evaluate\nLLM-as-a-Judge on alignment tasks by defining more theoretically interpretable\nevaluation metrics and explicitly mitigating LLM internal inconsistency from\nreliability metrics. We develop an open-source framework to evaluate, compare,\nand visualize the reliability and alignment of LLM judges, which facilitates\npractitioners to choose LLM judges for alignment tasks. In the experiments, we\nexamine effects of diverse prompt templates on LLM-judge reliability and also\ndemonstrate our developed framework by comparing various LLM judges on two\ncommon alignment datasets (i.e., TL;DR Summarization and HH-RLHF-Helpfulness).\nOur results indicate a significant impact of prompt templates on LLM judge\nperformance, as well as a mediocre alignment level between the tested LLM\njudges and human evaluators."
      }
    ],
    "LLVM": [
      {
        "title": "IRFuzzer: Specialized Fuzzing for LLVM Backend Code Generation",
        "summary": "Modern compilers, such as LLVM, are complex pieces of software. Due to their\ncomplexity, manual testing is unlikely to suffice, yet formal verification is\ndifficult to scale. End-to-end fuzzing can be used, but it has difficulties in\nachieving high coverage of some components of LLVM.\n  In this paper, we implement IRFuzzer to investigate the effectiveness of\nspecialized fuzzing of the LLVM compiler backend. We focus on two approaches to\nimprove the fuzzer: guaranteed input validity using constrained mutations and\nimproved feedback quality. The mutator in IRFuzzer is capable of generating a\nwide range of LLVM IR inputs, including structured control flow, vector types,\nand function definitions. The system instruments coding patterns in the\ncompiler to monitor the execution status of instruction selection. The\ninstrumentation not only provides a new coverage feedback called matcher table\ncoverage, but also provides an architecture specific guidance to the mutator.\n  We show that IRFuzzer is more effective than existing fuzzers by fuzzing on\n29 mature LLVM backend targets. In the process, we reported 74 confirmed new\nbugs in LLVM upstream, out of which 49 have been fixed, five have been back\nported to LLVM 15, showing that specialized fuzzing provides useful and\nactionable insights to LLVM developers."
      },
      {
        "title": "TroL: Traversal of Layers for Large Language and Vision Models",
        "summary": "Large language and vision models (LLVMs) have been driven by the\ngeneralization power of large language models (LLMs) and the advent of visual\ninstruction tuning. Along with scaling them up directly, these models enable\nLLVMs to showcase powerful vision language (VL) performances by covering\ndiverse tasks via natural language instructions. However, existing open-source\nLLVMs that perform comparably to closed-source LLVMs such as GPT-4V are often\nconsidered too large (e.g., 26B, 34B, and 110B parameters), having a larger\nnumber of layers. These large models demand costly, high-end resources for both\ntraining and inference. To address this issue, we present a new efficient LLVM\nfamily with 1.8B, 3.8B, and 7B LLM model sizes, Traversal of Layers (TroL),\nwhich enables the reuse of layers in a token-wise manner. This layer traversing\ntechnique simulates the effect of looking back and retracing the answering\nstream while increasing the number of forward propagation layers without\nphysically adding more layers. We demonstrate that TroL employs a simple layer\ntraversing approach yet efficiently outperforms the open-source LLVMs with\nlarger model sizes and rivals the performances of the closed-source LLVMs with\nsubstantial sizes."
      },
      {
        "title": "Phantom of Latent for Large Language and Vision Models",
        "summary": "The success of visual instruction tuning has accelerated the development of\nlarge language and vision models (LLVMs). Following the scaling laws of\ninstruction-tuned large language models (LLMs), LLVMs either have further\nincreased their sizes, reaching 26B, 34B, and even 80B parameters. While this\nincrease in model size has yielded significant performance gains, it demands\nsubstantially more hardware resources for both training and inference.\nConsequently, there naturally exists a strong need for efficient LLVMs that\nachieve the performance of larger models while being smaller in size. To\nachieve this need, we present a new efficient LLVM family with model sizes of\n0.5B, 1.8B, 3.8B, and 7B parameters, Phantom, which significantly enhances\nlearning capabilities within limited structures. By temporarily increasing the\nlatent hidden dimension during multi-head self-attention (MHSA), we make LLVMs\nprepare to look and understand much more vision-language knowledge on the\nlatent, without substantially increasing physical model sizes. To maximize its\nadvantage, we introduce Phantom Optimization (PO) using both autoregressive\nsupervised fine-tuning (SFT) and direct preference optimization (DPO)-like\nconcept, which effectively follows correct answers while eliminating incorrect\nand ambiguous ones. Phantom outperforms numerous larger open- and closed-source\nLLVMs, positioning itself as a leading solution in the landscape of efficient\nLLVMs."
      },
      {
        "title": "MoAI: Mixture of All Intelligence for Large Language and Vision Models",
        "summary": "The rise of large language models (LLMs) and instruction tuning has led to\nthe current trend of instruction-tuned large language and vision models\n(LLVMs). This trend involves either meticulously curating numerous instruction\ntuning datasets tailored to specific objectives or enlarging LLVMs to manage\nvast amounts of vision language (VL) data. However, current LLVMs have\ndisregarded the detailed and comprehensive real-world scene understanding\navailable from specialized computer vision (CV) models in visual perception\ntasks such as segmentation, detection, scene graph generation (SGG), and\noptical character recognition (OCR). Instead, the existing LLVMs rely mainly on\nthe large capacity and emergent capabilities of their LLM backbones. Therefore,\nwe present a new LLVM, Mixture of All Intelligence (MoAI), which leverages\nauxiliary visual information obtained from the outputs of external\nsegmentation, detection, SGG, and OCR models. MoAI operates through two newly\nintroduced modules: MoAI-Compressor and MoAI-Mixer. After verbalizing the\noutputs of the external CV models, the MoAI-Compressor aligns and condenses\nthem to efficiently use relevant auxiliary visual information for VL tasks.\nMoAI-Mixer then blends three types of intelligence (1) visual features, (2)\nauxiliary features from the external CV models, and (3) language features by\nutilizing the concept of Mixture of Experts. Through this integration, MoAI\nsignificantly outperforms both open-source and closed-source LLVMs in numerous\nzero-shot VL tasks, particularly those related to real-world scene\nunderstanding such as object existence, positions, relations, and OCR without\nenlarging the model size or curating extra visual instruction tuning datasets."
      }
    ]
  }
}